# Pixtral-12B Model Configuration

# Basic model information
name: "pixtral-12b"
repo_id: "mistral-community/pixtral-12b"
description: "Pixtral 12B vision-language model for invoice processing"
model_type: "LlavaForConditionalGeneration"
processor_type: "AutoProcessor"

# Hardware requirements
hardware:
  gpu_required: true
  gpu_memory_min: "24GB"
  recommended_gpu: "A4000 or better"
  minimum_compute_capability: "7.5"

# Loading configuration
loading:
  default_strategy: "optimized"
  default_params:
    torch_dtype: "bfloat16"
    device_map: "cuda:0"
    use_auth_token: false
    use_flash_attention_2: false
    attn_implementation: "eager"

# Quantization options
quantization:
  default: "bfloat16"
  options:
    bfloat16:
      torch_dtype: "bfloat16"
      device_map: "cuda:0"
      use_flash_attention_2: false
      attn_implementation: "eager"
    
    int8:
      load_in_8bit: true
      device_map: "auto"
      use_flash_attention_2: false
      attn_implementation: "eager"
      bnb_8bit_quant_type: "fp8"
    
    int4:
      load_in_4bit: true
      bnb_4bit_compute_dtype: "bfloat16"
      bnb_4bit_quant_type: "nf4"
      device_map: "auto"
      use_flash_attention_2: false
      attn_implementation: "eager"

# Prompt configuration
prompt:
  # Using direct format instead of chat template because:
  # 1. Our use case is single-image extraction
  # 2. We don't need conversation history
  # 3. Our prompts are relatively simple
  # 4. We're not doing multi-turn interactions
  # Chat template format would add unnecessary complexity
  format: "<s>[INST]Extract the work order number and total cost from this invoice image.\n[IMG][/INST>"
  image_placeholder: "[IMG]"
  default_field: "work_order"
  input_format: 
    - image: "RGB"
    - max_resolution: "4032x3024"
    - max_file_size_mb: 10

# Inference parameters
inference:
  max_new_tokens: 500
  do_sample: false
  temperature: 1.0
  top_k: 50
  top_p: 0.95
  batch_size: 1
  max_batch_memory_gb: 20

# Performance monitoring
performance:
  expected_accuracy: 0.7
  inference_timeout_seconds: 30
  gpu_utilization_threshold: 0.8

# Error handling
error_handling:
  retry_attempts: 2
  fallback_strategy: "minimal_params"
  critical_error_fields:
    - device_map
    - torch_dtype
    - trust_remote_code 
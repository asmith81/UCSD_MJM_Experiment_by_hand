# Pixtral-12B Model Configuration

# Basic model information
name: "pixtral-12b"
repo_id: "mistral-community/pixtral-12b"
description: "Pixtral 12B vision-language model for invoice processing"
model_type: "LlavaForConditionalGeneration"
processor_type: "AutoProcessor"

# Hardware requirements
hardware:
  gpu_required: true
  gpu_memory_min: 24  # GB as integer
  recommended_gpu: "A4000 or better"
  minimum_compute_capability: 7.5  # Float value
  disk_space_required: 50  # GB as integer
  ram_required: 32  # GB as integer

# Loading configuration
loading:
  default_strategy: "optimized"
  default_params:
    torch_dtype: "bfloat16"
    device_map: "cuda:0"
    use_auth_token: false
    use_flash_attention_2: false
    attn_implementation: "eager"
  validation:
    max_batch_size: 1
    timeout_seconds: 300
    retry_attempts: 3

# Quantization options
quantization:
  default: "bfloat16"
  options:
    bfloat16:
      torch_dtype: "bfloat16"
      device_map: "cuda:0"
      use_flash_attention_2: false
      attn_implementation: "eager"
      memory_usage: 24  # GB as integer
    
    int8:
      load_in_8bit: true
      device_map: "auto"
      use_flash_attention_2: false
      attn_implementation: "eager"
      bnb_8bit_quant_type: "fp8"
      memory_usage: 12  # GB as integer
    
    int4:
      load_in_4bit: true
      bnb_4bit_compute_dtype: "bfloat16"
      bnb_4bit_quant_type: "nf4"
      device_map: "auto"
      use_flash_attention_2: false
      attn_implementation: "eager"
      memory_usage: 6  # GB as integer

# Prompt configuration
prompt:
  # Using direct format instead of chat template because:
  # 1. Our use case is single-image extraction
  # 2. We don't need conversation history
  # 3. Our prompts are relatively simple
  # 4. We're not doing multi-turn interactions
  # Chat template format would add unnecessary complexity
  format: "<s>[INST]Extract the work order number and total cost from this invoice image.\n[IMG][/INST>"
  image_placeholder: "[IMG]"
  default_field: "work_order"
  input_format: 
    image: "RGB"
    max_resolution: "4032x3024"
    max_file_size_mb: 10
  validation:
    required_fields: ["work_order", "total_cost"]
    field_types:
      work_order: "string"
      total_cost: "float"
    min_confidence: 0.7

# Inference parameters
inference:
  max_new_tokens: 512
  temperature: 0.1
  top_p: 0.9
  repetition_penalty: 1.1
  do_sample: true
  validation:
    timeout_seconds: 30
    max_retries: 3
    min_confidence: 0.7
    required_fields: ["work_order", "total_cost"]

# Performance monitoring
performance:
  expected_accuracy: 0.7
  inference_timeout_seconds: 30
  gpu_utilization_threshold: 0.8

# Error handling
error_handling:
  retry_strategy:
    max_attempts: 3
    backoff_factor: 2
    initial_delay: 1
  fallback_strategy:
    use_simpler_model: true
    use_cached_results: true
    timeout_action: "return_partial"
  logging:
    level: "INFO"
    format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    max_file_size_mb: 10
    backup_count: 5
  critical_error_fields:
    - device_map
    - torch_dtype
    - trust_remote_code 